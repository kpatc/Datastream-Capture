# CDC Real-time Data Warehousing

> Real-time Change Data Capture (CDC) from PostgreSQL to Snowflake via Debezium & Kafka

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Kafka](https://img.shields.io/badge/kafka-6.2.0-orange.svg)](https://kafka.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/postgresql-14+-blue.svg)](https://www.postgresql.org/)
[![Snowflake](https://img.shields.io/badge/snowflake-cloud-blue.svg)](https://www.snowflake.com/)

## ğŸ¯ Description

Professional Change Data Capture (CDC) pipeline that captures real-time changes in PostgreSQL and automatically synchronizes them to Snowflake for data warehousing.

### Architecture

```
PostgreSQL â†’ Debezium â†’ Kafka â†’ Python Pipeline â†’ Snowflake
                â†“
          Schema Registry
```

## âœ¨ Features

### Core CDC Features
- âœ… **Real-time CDC capture**: Instantly detects INSERT, UPDATE, DELETE operations
- âœ… **Automatic validation**: Verifies data quality before loading
- âœ… **Micro-batch processing**: Optimizes performance with configurable batching
- âœ… **MERGE management**: Intelligent upsert/delete in Snowflake

### Production-Ready Robustness
- âœ… **Circuit Breaker Pattern**: Prevents cascade failures for Kafka and Snowflake
- âœ… **Exponential Backoff Retry**: Automatic retry with intelligent backoff (3-5 attempts)
- âœ… **Dead Letter Queue (DLQ)**: Captures failed messages for analysis and replay
- âœ… **Error Categorization**: Tracks Kafka, Snowflake, validation, and parsing errors
- âœ… **Graceful Shutdown**: Signal handling for clean pipeline termination

### Monitoring & Observability
- âœ… **Comprehensive Metrics**: Throughput, success rate, error rate, latency
- âœ… **Real-time Statistics**: Live tracking of messages consumed, validated, loaded
- âœ… **Performance Analytics**: Batch processing times, average batch sizes
- âœ… **CDC Operation Tracking**: Monitors CREATE, UPDATE, DELETE, READ operations
- âœ… **Metrics Export**: JSON export for integration with monitoring tools
- âœ… **Structured Logging**: Production-ready logging with proper levels

### Quality & Testing
- âœ… **Unit tests**: Comprehensive test coverage > 80%
- âœ… **Integration tests**: End-to-end pipeline validation
- âœ… **Makefile automation**: One-command testing and deployment

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- Docker & Docker Compose
- Snowflake account

### 5-Minute Installation

```bash
# 1. Clone the project
git clone https://github.com/kpatc/Datastream-Capture.git
cd Datastream-Capture

# 2. Create virtual environment
make venv
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate  # Windows

# 3. Install dependencies
make install

# 4. Configure Snowflake
cp .env.example .env
nano .env  # Add your Snowflake credentials

# 5. Start Docker services
make docker-up

# 6. Create Debezium connector
make create-connector

# 7. Setup Snowflake
make setup-snowflake

# 8. Run the pipeline
make pipeline
```

**That's it! ğŸ‰** The pipeline is now capturing changes in real-time.

## ğŸ“– Complete Documentation

For a detailed installation and configuration guide, see:

ğŸ‘‰ **[SETUP_GUIDE.md](docs/SETUP_GUIDE.md)**

This guide contains:
- Detailed architecture
- Step-by-step configuration
- Troubleshooting
- Optimizations
- Usage examples

## ğŸ§ª Tests

```bash
# Run all tests
make test

# Tests with coverage
make coverage

# Specific tests
make test-kafka        # Kafka consumer tests
make test-snowflake    # Snowflake connector tests
make test-pipeline     # Pipeline integration tests
```

## ğŸ”§ Useful Commands

```bash
# Development
make help              # Display all commands
make format            # Format code
make lint              # Check code quality
make dev               # Run in development mode

# Docker
make docker-up         # Start all services
make docker-down       # Stop services
make docker-logs       # View logs

# Monitoring
make health-check      # Check service status
make check-connector   # Debezium connector status

# Testing
make test-connection   # Test Snowflake connection
make generate-test-data # Generate test data

# Cleanup
make clean             # Clean cache
make clean-all         # Clean everything (cache + Docker)
```

## ğŸ“Š Available Services

Once `make docker-up` is executed:

| Service | URL | Description |
|---------|-----|-------------|
| Kafka | localhost:9092 | Message broker |
| Kafka Connect | http://localhost:8083 | Debezium API |
| Kowl (Kafka UI) | http://localhost:8090 | Kafka interface |
| PostgreSQL | localhost:5433 | Source database |
| Adminer | http://localhost:7775 | PostgreSQL interface |
| Schema Registry | http://localhost:8081 | Avro/JSON schema |

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ pipeline.py              # Main pipeline
â”‚   â”œâ”€â”€ kafka_consumer.py        # Kafka consumer + Debezium parsing
â”‚   â”œâ”€â”€ snowflake_connector.py   # Snowflake connection
â”‚   â””â”€â”€ config.py                # Centralized configuration
â”œâ”€â”€ tests/                        # Unit tests
â”‚   â”œâ”€â”€ test_kafka_consumer.py
â”‚   â”œâ”€â”€ test_snowflake_connector.py
â”‚   â””â”€â”€ test_pipeline.py
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ setup_snowflake.py       # Snowflake DB setup
â”‚   â””â”€â”€ test_pipeline.py         # Test data generation
â”œâ”€â”€ docker/                       # CDC infrastructure
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ transaction_connector.json
â”œâ”€â”€ docs/                         # Documentation
â”‚   â””â”€â”€ SETUP_GUIDE.md
â”œâ”€â”€ Makefile                      # Automated commands
â””â”€â”€ requirements.txt              # Python dependencies
```

## ğŸ”¥ Typical Workflow

### 1. Daily development

```bash
# Start environment
make docker-up
make pipeline

# In another terminal: generate transactions
make generate-test-data

# Check in Snowflake
# Data arrives in real-time!
```

### 2. Before committing

```bash
# Format and check
make all

# If everything is âœ…, commit!
git add .
git commit -m "feat: pipeline improvement"
```

### 3. CI/CD

```bash
# Run CI checks
make ci

# Lint + Automatic tests
```

## ğŸ“ˆ Metrics & Monitoring

The pipeline displays in real-time:

```
Pipeline Stats - Consumed: 15230, Validated: 15100, Loaded: 15000, Errors: 2
Total records in Snowflake: 15000
```

## ğŸ› Troubleshooting

### Connector won't start

```bash
make check-connector
docker logs kafka-connect
```

### No messages in Kafka

```bash
# Check PostgreSQL WAL level
docker exec postgres psql -U postgres -c "SHOW wal_level;"
# Should be: logical
```

### Snowflake error

```bash
make test-connection
```

### Pipeline stuck

```bash
# Check Kafka offsets
make check-connector
```

More details in [SETUP_GUIDE.md](docs/SETUP_GUIDE.md)

## ğŸ“ Configuration

### Snowflake (`.env`)

```env
SNOWFLAKE_ACCOUNT=xyz.region
SNOWFLAKE_USER=username
SNOWFLAKE_PASSWORD=password
SNOWFLAKE_DATABASE=CDC_DB
SNOWFLAKE_SCHEMA=PUBLIC
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
```

### Pipeline

- `BATCH_SIZE=100`: Number of messages per batch
- `BATCH_TIMEOUT=5`: Batch timeout in seconds
- `KAFKA_TOPIC=postgres-transactions.public.transactions`

## ğŸ¤ Contributing

1. Fork the project
2. Create a branch: `git checkout -b feature/amazing-feature`
3. Commit: `git commit -m 'feat: Add amazing feature'`
4. Push: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“œ License

This project is licensed under the MIT License - see [LICENSE](LICENSE)

## ğŸ‘¥ Authors

- **Josh** - *Initial work* - [kpatc](https://github.com/kpatc)

## ğŸ™ Acknowledgments

- [Debezium](https://debezium.io/) - CDC platform
- [Apache Kafka](https://kafka.apache.org/) - Streaming
- [Snowflake](https://www.snowflake.com/) - Data warehouse

---

**â­ If this project helps you, feel free to give it a star!**

**ğŸ“§ Questions? Open an issue!**
